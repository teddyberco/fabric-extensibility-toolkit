# Excel Edit Item - Fabric-Native Excel Editing

## Overview

The ExcelEdit item provides a complete Fabric-native workflow for editing Lakehouse data with Excel capabilities, all without leaving the Microsoft Fabric environment.

## Features

### ğŸ  **Fabric-Native Architecture**
- **DataHub Integration**: Select lakehouses using Fabric's DataHub SDK
- **Connected Workbooks**: Edit data using `@microsoft/connected-workbooks` 
- **OneLake Storage**: Save edited data directly to OneLake folders
- **No External Navigation**: Complete experience within Fabric

### ğŸ“Š **Workflow Steps**

1. **Lakehouse Selection** 
   - Browse available lakehouses via DataHub SDK
   - Filter by workspace and permissions
   - Mock data includes Sales, Customer Analytics, and Inventory lakehouses

2. **Table Selection**
   - View table metadata (columns, row count, last modified)
   - Preview table schema and data types
   - Support for multiple data types (string, decimal, datetime, int)

3. **Excel Creation with Schema Preservation**
   - Query lakehouse table via Spark Livy API
   - Extract original schema for type-aware operations
   - Generate Excel file with real data (up to 1000 rows)
   - Professional styling with Microsoft blue headers

4. **Excel Editing**
   - Integration with Excel Online via OneDrive for Business
   - In-Fabric Excel editing capabilities
   - Live data preview and manipulation
   - Professional Excel experience without leaving Fabric

5. **OneLake Storage**
   - Save edited data to user's OneLake folders
   - Folder browsing and selection
   - Automatic file naming with timestamps
   - Preservation of data formatting

6. **Save to Lakehouse with Type Preservation**
   - Parse Excel file client-side using ExcelJS
   - Validate schema compatibility with original table
   - Convert Excel strings to proper Spark types:
     - Boolean: `"true"` â†’ `True`, `"false"` â†’ `False`
     - Integer/Long: `"42"` â†’ `42`
     - Float/Double: `"3.14"` â†’ `3.14`
     - Timestamp: `"2023-01-01T00:00:00"` â†’ Python datetime
     - String: Preserved as-is
   - Execute SQL INSERT OVERWRITE for Delta Lake compatibility
   - Preserve Delta Lake transaction history

### ğŸ”§ **Technical Implementation**

#### Key Components
- **ExcelEditItemEditorDefault.tsx**: Main workflow component with state management
- **ExcelClientSide.ts**: Client-side Excel generation and lakehouse write operations
- **SparkQueryHelper.ts**: Spark Livy session management and query execution
- **WorkflowState enum**: State management for multi-step process
- **LakehouseInfo/TableInfo/OneLakeFolder interfaces**: TypeScript type safety

#### State Management
```typescript
enum WorkflowState {
  INITIAL = 'initial',
  SELECTING_LAKEHOUSE = 'selecting_lakehouse', 
  SELECTING_TABLE = 'selecting_table',
  LOADING_DATA = 'loading_data',
  EXCEL_EDITING = 'excel_editing',
  SAVING_TO_ONELAKE = 'saving_to_onelake',
  COMPLETED = 'completed'
}
```

#### Data Flow

**Read Path (Lakehouse â†’ Excel):**
1. **DataHub SDK** â†’ List lakehouses and tables
2. **Spark Livy API** â†’ Query table data with schema extraction
3. **SparkQueryHelper** â†’ Execute PySpark code and poll for results
4. **ExcelClientSide** â†’ Generate Excel file with styled headers
5. **Excel Online** â†’ Display in embedded iframe for editing

**Write Path (Excel â†’ Lakehouse):**
1. **Graph API** â†’ Download Excel file using `@microsoft.graph.downloadUrl`
2. **ExcelJS** â†’ Parse Excel binary data client-side
3. **Type Conversion** â†’ Convert Excel strings to Spark types
4. **Spark Livy API** â†’ Create DataFrame with original schema
5. **SQL INSERT OVERWRITE** â†’ Write to Delta Lake table via temp view

### ğŸš€ **Usage Scenario**

**Business Analyst Workflow:**
1. **Create ExcelEdit Item**: User creates new ExcelEdit item in their workspace
2. **Connect to Data**: Select relevant lakehouse (e.g., "Sales Analytics")
3. **Choose Dataset**: Pick table (e.g., "Monthly Sales Performance") 
4. **Edit in Excel**: Use familiar Excel interface within Fabric
5. **Save Results**: Store edited data to OneLake folder for sharing

### ğŸ“¦ **Dependencies**

- `exceljs`: Client-side Excel file parsing and generation
- `@ms-fabric/workload-client`: Fabric SDK integration
- `@fluentui/react-components`: UI components
- **Fabric REST APIs**:
  - Lakehouse API: Get lakehouse properties and table lists
  - Spark Livy API: Session management and query execution
  - Microsoft Graph API: Excel file downloads and OneDrive integration
- **DataHub SDK**: Lakehouse discovery
- **OneLake SDK**: File storage integration

### ğŸ›  **Development Notes**

#### Current Implementation
- âœ… **Real lakehouse data** queried via Spark Livy API
- âœ… **Schema preservation** from lakehouse to Excel and back
- âœ… **Type conversion** for Boolean, Long, Double, String, Timestamp, Date types
- âœ… **Excel Online integration** via OneDrive for Business
- âœ… **SQL INSERT OVERWRITE** for Delta Lake compatibility
- âœ… **Client-side Excel processing** - no backend required for parsing
- âœ… **Error handling** with validation and detailed error messages
- Progress indicator with workflow steps

#### Technical Details

**Type Conversion Logic:**
```typescript
// Boolean: Excel "true"/"false" strings â†’ Python True/False
if (field.type === 'BooleanType') {
  convertedValue = value.toLowerCase() === 'true';
}

// Integer/Long: Excel number strings â†’ Python int
if (field.type === 'IntegerType' || field.type === 'LongType') {
  convertedValue = parseInt(value, 10);
}

// Float/Double: Excel decimal strings â†’ Python float
if (field.type === 'FloatType' || field.type === 'DoubleType') {
  convertedValue = parseFloat(value);
}

// Timestamp: ISO 8601 strings â†’ Python datetime
if (field.type === 'TimestampType') {
  convertedValue = `datetime.fromisoformat("${value}")`;
}
```

**SQL INSERT OVERWRITE Approach:**
```python
# Create DataFrame with original schema
df = spark.createDataFrame(data, schema)

# Create temporary view for SQL operation
df.createOrReplaceTempView("temp_insert_view")

# Use INSERT OVERWRITE for Delta Lake compatibility
spark.sql(f"INSERT OVERWRITE TABLE {table_name} SELECT * FROM temp_insert_view")
```

#### Production Roadmap
1. âœ… **DataHub SDK Integration**: Real lakehouse discovery implemented
2. âœ… **Excel Online Integration**: Full Excel editing via OneDrive
3. âœ… **Lakehouse Write**: SQL INSERT OVERWRITE with type preservation
4. âœ… **Error Handling**: Comprehensive validation and error messages
5. ğŸ”„ **Performance Optimization**: Consider pagination for tables > 1000 rows

### ğŸ“‹ **Configuration**

The ExcelEdit item is configured in:
- **Environment**: `.env.dev` - `ITEM_NAMES=HelloWorld,ExcelEdit`
- **Manifest**: `Product.json` - Card configuration
- **Routing**: `App.tsx` - Route registration
- **Translations**: Multiple language support

### ğŸ” **Testing**

Access the ExcelEdit item workflow:
1. Start development server: `npm run start`
2. Open browser: `http://localhost:60006`
3. Navigate to ExcelEdit item
4. Follow the guided workflow steps

### ğŸ¯ **Business Value**

- **Familiar Interface**: Excel editing experience analysts know
- **Secure Environment**: No data leaves Fabric ecosystem  
- **Integrated Workflow**: Seamless lakehouse-to-analysis pipeline
- **Collaborative**: OneLake storage enables team sharing
- **Governed**: Fabric security and compliance maintained